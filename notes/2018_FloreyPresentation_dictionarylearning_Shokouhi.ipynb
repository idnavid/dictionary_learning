{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Dictionary Learning for Data Driven Analysis of Neuroimaging Signals\n",
    "Presenter: Navid Shokouhi\n",
    "\n",
    "*The University of Melbourne*\n",
    "\n",
    "work in collaboration with Dr. Abd-Krim Seghouane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "- Presenter Background\n",
    "    - Previous Work\n",
    "- Background on Sparse Dictionary Learning\n",
    "- Preliminary Results\n",
    "- Directions of Future Work \n",
    "- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Navid Shokouhi\n",
    "PhD in *Statistical Signal Processing* and *Machine Learning*\n",
    "- *The University of Texas at Dallas, USA*\n",
    "- Worked on machine learning for speech applications: spoken dialogue systems\n",
    "\n",
    "Postdoc in *Statistical Signal Processing* and *Machine Learning*\n",
    "- *The University of Melbourne*\n",
    "- functional data analysis for Neuroimaging\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Research Interests:\n",
    "    - Statistical interpretation of Machine Learning Algorithms\n",
    "    - Multivariate statistical analysis: PCA, ICA, CCA, LDA\n",
    "    - Sparse Statistical Learning \n",
    "    \n",
    "    \n",
    "- Research Experience:\n",
    "    - Machine learning for data driven Neuroimaging analysis: \n",
    "        - fMRI, fNIRS\n",
    "    - Speech Recognition and Voice biometrics\n",
    "        - PhD dissertation\n",
    "\n",
    "\n",
    "- Professional (non-academic) Experience:\n",
    "    - *Pull String Inc., San Francisco, CA, USA*\n",
    "        - Software developer in applied computational linguistics\n",
    "        - Python/C++\n",
    "    - Work in collaboration with *Samsung, USA*\n",
    "        - Speech Recognition for Smart TVs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Previous Work in Neuroimaging:\n",
    "- Model order selection for ICA (resting-state fMRI analysis)\n",
    "- Model order selection for CCA (multi-modal data analysis)\n",
    "- Robust Estimation of the Haemodynamic Response Function\n",
    "- Sparse PCA as a preprocessing step for ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Background: Model Order Selection\n",
    "There are several perspectives that can be used to fit the data: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Kullback- Leibler divergence can be used as a measure of discrepency between the true distritution corresponding to $\\boldsymbol{\\theta}_q$ and the candidate probability density functions:\n",
    "$$L(\\boldsymbol{\\theta}_k)\\propto KL(f({\\bf X}|\\boldsymbol{\\theta}_q)||f({\\bf X}|\\boldsymbol{\\theta}_k))$$\n",
    "- $L(\\boldsymbol{\\theta}_k)$ is called the goodness-of-fit\n",
    "- Akaike's information criterion (AIC) and KIC takes this approach. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The maximum a posteriori probability can be calculated in the case of Bayesian arguements.\n",
    "- Bayesian information criterion (BIC) takes this approach. \n",
    "- It's goodness-of-fit is similar to that of AIC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The idea of universal coding introduced by Kolmogrov is used in minimum description length. \n",
    "- It's goodness-of-fit is similar to that of AIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above mentioned criteria are commonly referred to as *information theoretic criteria*. \n",
    "\n",
    "The goodness-of-fit for all of the information theoretic criteria is the log of the likelihood function. \n",
    "$$\\log (f({\\bf X}|\\boldsymbol{\\theta}_k))$$\n",
    "\n",
    "Of course, the goodness-of-fit monotically increases with $k$. \n",
    "\n",
    "Therefore, a penalty term is required to penalize complex models. \n",
    "\n",
    "*Model Selection criteria (i.e., information theoretic criteria) essentially leverage complexity with the goodness-of-fit and all take the following form:*\n",
    "$$IC(k) = \\log (f({\\bf X}|\\boldsymbol{\\theta}_k)) + P(k,n)$$\n",
    "\n",
    "The penalty is a function of the candidate dimensionality $k$ and the number of observations $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Under the conditions (namely zero-mean i.i.d Gaussian data with white noise):\n",
    "$$\\log (f({\\bf X}|\\boldsymbol{\\theta}_k)) \\propto \\sum_{i=k+1}^{p}\\log(\\hat{\\lambda}_i) - (p-k)\\log(\\frac{1}{p-k}\\sum_{i=k+1}^{p}\\hat{\\lambda}_i)$$\n",
    "where $\\hat{\\lambda}_{1}\\ge\\hat{\\lambda}_{2}\\ge\\dots\\ge\\hat{\\lambda}_p$ are the eigenvalues of the sample covariance matrix $\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n-1}{\\bf X}^\\top{\\bf X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
